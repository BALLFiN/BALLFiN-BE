from app.models.llm.DB_loader import load_vectordb
from app.models.llm.graph import create_langgraph
from langchain_core.messages import HumanMessage
from typing import Dict

# ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ë„êµ¬ í•¨ìˆ˜
def query_news(query: str) -> Dict:
    """ë‰´ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¿¼ë¦¬í•˜ê³  LangGraphë¥¼ ì‚¬ìš©í•´ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë„êµ¬"""
    # VectorDB ë¡œë“œ
    vectordb = load_vectordb()
    if not vectordb:
        return {"status": "error", "message": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    # ê²€ìƒ‰ê¸° ì„¤ì •
    retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 10})
    
    # LangGraph ìƒì„±
    graph = create_langgraph(retriever)
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        "messages": [HumanMessage(content=query)],
        "query": query,
    }
    
    try:
        print("ğŸ“¤ Graph ì‹¤í–‰ ì¤‘...")
        # ê·¸ë˜í”„ ì‹¤í–‰
        result = graph.invoke(initial_state)
        
        # ê´€ë ¨ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        sources = []
        if "context" in result and result["context"]:
            for doc in result["context"]:
                sources.append(doc.metadata)
        
        return {
            "status": "success",
            "response": result["response"],
            "sources": sources
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        error_message = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        print(error_message)
        return {"status": "error", "message": error_message}

